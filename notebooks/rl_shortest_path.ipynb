{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoutingEnv(object):\n",
    "    def __init__(self, g, init_state, target_state, keep_hist=False):\n",
    "        self.g = g\n",
    "        self.all_states = list(self.g.nodes)\n",
    "        self.all_actions = list(self.g.edges)\n",
    "        self.s = init_state\n",
    "        self.t = target_state\n",
    "        self.keep_hist = keep_hist\n",
    "        self.start()\n",
    "    \n",
    "    def start(self):\n",
    "        self.state = self.s\n",
    "        self.act_hist = []\n",
    "        self.state_hist = []\n",
    "        self.hist = dict(state=[], act=[])\n",
    "        \n",
    "    def reset(self):\n",
    "        if self.keep_hist:\n",
    "            self.episode_hist['state'].append(self.state_hist)\n",
    "            self.episode_hist['act'].append(self.act_hist)\n",
    "        \n",
    "        self.state = self.s\n",
    "        self.act_hist = []\n",
    "        self.state_hist = []\n",
    "    \n",
    "    def step(self, a):\n",
    "        '''\n",
    "            a: an edge (pair of nodes) in the graph g\n",
    "        '''\n",
    "        assert a[0] == self.state\n",
    "        assert a in self.g.edges\n",
    "        \n",
    "        repeated_action = a in self.act_hist\n",
    "    \n",
    "        self.state = a[1]\n",
    "        if self.keep_hist:\n",
    "            self.state_hist.append(self.state)\n",
    "            self.act_hist.append(a)\n",
    "\n",
    "        is_deadend = len(list(self.g.neighbors(self.state))) == 0\n",
    "        episode_over = self.state == self.t or is_deadend\n",
    "\n",
    "        # test this first since the correct target can also be a deadend\n",
    "        if self.state == self.t:\n",
    "            reward = 100\n",
    "        elif is_deadend:\n",
    "            reward = -100\n",
    "        elif repeated_action:\n",
    "            reward = -5\n",
    "        else:\n",
    "            reward = -1\n",
    "      \n",
    "        return reward, episode_over, self.state\n",
    "    \n",
    "    def render(self, ax=None):\n",
    "        non_terminal = self.g.nodes - set([self.s, self.t])\n",
    "        nx.draw_networkx(\n",
    "            self.g, pos=self.g.my_pos, nodelist=[self.s], edgelist=[], node_color='y', ax=ax\n",
    "        )\n",
    "        nx.draw_networkx(\n",
    "            self.g, pos=self.g.my_pos, nodelist=[self.t], edgelist=[], node_color='c', ax=ax\n",
    "        )\n",
    "        nx.draw_networkx(self.g, pos=self.g.my_pos, nodelist=non_terminal, ax=ax)\n",
    "        nx.draw_networkx_edges(\n",
    "            self.g, pos=self.g.my_pos, edgelist=self.act_hist, width=8, edge_color='r', ax=ax\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = [\n",
    "    (6, 7), (7, 8), (6, 3), (7, 4), (8, 5),\n",
    "    (3, 4), (4, 5), (3, 0), (4, 1), (5, 2),\n",
    "    (0, 1), (1, 2),\n",
    "]\n",
    "rev_edges = [(j, i) for i, j in edges]\n",
    "edges = edges + rev_edges\n",
    "edges = edges + [\n",
    "    (0, 'i0'), ('o0', 0), (1, 'i1'), ('o1', 1),\n",
    "    (2, 'i2'), ('o2', 2), (3, 'i3'), ('o3', 3),\n",
    "    (5, 'i5'), ('o5', 5), (6, 'i6'), ('o6', 6),\n",
    "    (7, 'i7'), ('o7', 7), (8, 'i8'), ('o8', 8),\n",
    "]\n",
    "edges = [(str(i), str(j)) for i, j in edges]\n",
    "\n",
    "# cartesian frame, i.e. (x, y) pairs, with origin at bottom left\n",
    "pos = {\n",
    "    '0': (0, 0), '1': (1, 0), '2': (2, 0), '3': (0, 1),\n",
    "    '4': (1, 1), '5': (2, 1), '6': (0, 2), '7': (1, 2),\n",
    "    '8': (2, 2), 'i0': (-.4, -.8), 'o0': (-.8, -.4), \n",
    "    'i1': (.8, -1), 'o1': (1.2, -1), \n",
    "    'i2': (2.6, -.4), 'o2': (2.4, -.6),\n",
    "    'i3': (-1, .8), 'o3': (-1, 1.2),\n",
    "    'i5': (3, .8), 'o5': (3, 1.2),\n",
    "    'i6': (-.4, 2.6), 'o6': (-.6, 2.4),\n",
    "    'i7': (.8, 3), 'o7': (1.2, 3),\n",
    "    'i8': (2.4, 2.6), 'o8': (2.6, 2.4),\n",
    "}\n",
    "super_graph = nx.DiGraph(edges)\n",
    "super_graph.my_pos = pos\n",
    "env = RoutingEnv(super_graph, 'o0', 'i7')\n",
    "env.step(('o0', '0'))\n",
    "env.step(('0', '3'))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aa  = pd.DataFrame(np.arange(12).reshape(6, 2), index=[0 ,1 ,2, 3, 'o', 'i'], columns=[(1, 2), ('o', 'i')])\n",
    "aa.loc[0].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ùõº = .01\n",
    "ùõæ = .9\n",
    "ùúñ = .1  # prob explore\n",
    "\n",
    "env = RoutingEnv(super_graph, 'o0', 'i7')\n",
    "\n",
    "# using a tuple/edge as columns confuses pandas, convert to strings:\n",
    "all_actions_str = [f'{i}_{j}' for i, j in env.all_actions]\n",
    "q = pd.DataFrame(0, index=env.all_states, columns=all_actions_str)\n",
    "\n",
    "env.start()\n",
    "reward_hist = []\n",
    "for ep in tqdm.tqdm(range(100)):\n",
    "    episode_over = False\n",
    "    ep_reward = 0\n",
    "    while not episode_over:\n",
    "        act_type = np.random.choice(['explore', 'exploit'], p=[ùúñ, 1 - ùúñ])\n",
    "        \n",
    "        avail_actions = list(env.g.edges(env.state))\n",
    "        if act_type == 'explore':\n",
    "            # choice() is fussy:| a list of tuples looks like a 2d array to it which it doesn't like\n",
    "            ind = np.random.choice(np.arange(len(avail_actions)))\n",
    "            act = avail_actions[ind]\n",
    "            act_str = f'{act[0]}_{act[1]}'\n",
    "        else:  # 'exploit'\n",
    "            # TODO is pandas efficient enough?\n",
    "            # TODO is it necessary slice by avail_inds?\n",
    "            # this is a row slice i.e. a row as a Series, therefore argmax returns the col\n",
    "            avail_actions_str = [f'{i}_{j}' for i, j in avail_actions]\n",
    "            act_str = q.loc[env.state, avail_actions_str].idxmax()\n",
    "            act = act_str.split('_')\n",
    "\n",
    "        old_state = env.state\n",
    "        reward, episode_over, _ = env.step(act)\n",
    "        \n",
    "        # TODO is it necessary slice by avail_inds?\n",
    "        avail_actions = list(env.g.edges(env.state))\n",
    "        if avail_actions:\n",
    "            avail_actions_str = [f'{i}_{j}' for i, j in avail_actions]\n",
    "            exp_q_cur_state = q.loc[env.state, avail_actions_str].max()\n",
    "        else:\n",
    "            # TODO if we get rid of avail_actions in the slicing, we don't need this if since\n",
    "            #      at a deadend state e.g. 'i0', no actions are allowed so that column remains\n",
    "            #      zero (since we initialize q as 0 matrix)\n",
    "            exp_q_cur_state = 0\n",
    "        \n",
    "        q.loc[old_state, act_str] = (1 - ùõº) * q.loc[old_state, act_str] + ùõº * (reward + ùõæ * exp_q_cur_state)\n",
    "        assert not q.isna().any().any()\n",
    "        \n",
    "        ep_reward += reward\n",
    "    # fig, ax = plt.subplots()\n",
    "    # env.render(ax=ax)\n",
    "    reward_hist.append(ep_reward)\n",
    "    env.reset()\n",
    "plt.plot(reward_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.keep_hist = True\n",
    "episode_over = False\n",
    "ep_reward = 0\n",
    "env.start()\n",
    "count = 0\n",
    "while not episode_over and count < 15:\n",
    "    count += 1\n",
    "    avail_actions = list(env.g.edges(env.state))\n",
    "    avail_actions_str = [f'{i}_{j}' for i, j in avail_actions]\n",
    "    act_str = q.loc[env.state, avail_actions_str].idxmax()\n",
    "    act = act_str.split('_')\n",
    "\n",
    "    reward, episode_over, _ = env.step(act)\n",
    "    ep_reward += reward\n",
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
